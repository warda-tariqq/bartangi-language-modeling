
# Bartangi Language Corpus and Embeddings Project

Prepared by: **Warda**  
Supervisors: **Victor Popov**, **Vasilii Gromov**

---

## 1. Raw Corpus

- **Folder:** `raw_corpus/`
- **Description:**  
  Original unprocessed Bartangi language sentences collected from available sources.
- **File Type:**  
  `.txt` files, each containing one raw sentence.

---

## 2. Processing Pipeline

- **Cleaning:**
  - Removed unwanted punctuation (e.g., commas inside words, brackets).
  - Tokenized sentences carefully, preserving meaningful word boundaries.
  - Ensured only clean, valid words are kept for lemmatization.

- **Lemmatization:**
  - Used **Uniparser-Morph-Bartangi** morphological analyzer.
  - Link to lemmatizer repository: https://github.com/Novokshanov/uniparser-morph-bartangi
  - Fully respects morphological structure (prefixes, stems, suffixes, clitics).

---

## 3. Processed Corpus

- **Folder:** `lemmatized_corpus/`
- **Description:**  
  Cleaned and linguistically accurate lemmatized Bartangi sentences.
- **Format:**  
  One file per sentence (`sentence_001.txt`, `sentence_002.txt`, etc.), containing lemmas.

---

## 4. Word Embeddings

- **Models:**
  - `bartangi_word2vec.model` â†’ Skip-gram (sg=1) Word2Vec embeddings
  - `bartangi_word2vec_cbow.model` â†’ CBOW (sg=0) Word2Vec embeddings

- **Training Settings:**
  - **Vector Size:** 100 dimensions
  - **Window Size:** 5
  - **Minimum Word Frequency:** 1
  - **Architecture:** Skip-gram and CBOW both trained
  - **Tool:** Gensim Word2Vec

- **Output Files:**
  - `bartangi_word2vec.model`
  - `bartangi_word2vec_cbow.model`
  - `bartangi_cbow_vectors.csv` (CBOW vectors saved for analysis)

---

## 5. Comparison Between Skip-gram and CBOW

- Performed comparison of similar words generated by both models.
- Observed:
  - **Skip-gram**: Better for context-sensitive and rare words.
  - **CBOW**: Smoother embeddings, focused on frequent co-occurrences.
- Visualization using PCA plotted the embeddings of both models on a 2D plane.

---

## 6. Visualizations

- Plotted PCA and t-SNE visualizations of:
  - Skip-gram embeddings
  - CBOW embeddings
  - Combined Skip-gram vs CBOW in one graph for comparison
- Visualizations demonstrate clustering behavior and model differences.

---

## 7. Folder Structure

bartangi_corpus_project/
â”œâ”€â”€ raw_corpus/                 # Raw text files
â”œâ”€â”€ lemmatized_corpus/           # Cleaned and lemmatized text files
â”œâ”€â”€ bartangi_word2vec.model      # Skip-gram Word2Vec model
â”œâ”€â”€ bartangi_word2vec_cbow.model # CBOW Word2Vec model
â””â”€â”€ bartangi_cbow_vectors.csv    # CBOW word vectors in CSV


---

## ðŸ“ˆ Summary

This project presents a complete end-to-end pipeline for Bartangi language resource preparation:  
Raw corpus â†’ Cleaning â†’ Lemmatization â†’ Embedding training â†’ Comparison â†’ Visualization.

âœ… Ready for further linguistic, computational, and research analyses!
